library(data.table)
library(xgboost)
library(caret)

# make this example reproducible
set.seed(0)

encoded_train_table <- data.table::fread("/kaggle/input/housing-data-preprocessed-data/encoded_train.csv", index = "Id")
encoded_test_table <- data.table::fread("/kaggle/input/housing-data-preprocessed-data/encoded_test.csv", index = "Id")

nan_proportions <- sapply(
  encoded_train_table, function(x) sum(is.na(x)) / length(x)
)
valid_cols <- names(encoded_train_table)[nan_proportions <= 0.1]
encoded_train_table <- encoded_train_table[, ..valid_cols]
valid_cols <- setdiff(valid_cols, "SalePrice")
encoded_test_table <- encoded_test_table[, ..valid_cols]

train_x <- encoded_train_table[, -c("Id", "SalePrice")]
train_y <- encoded_train_table[["SalePrice"]]
test_x <- encoded_test_table[, -c("Id")]

xgb_train <- xgb.DMatrix(data = as.matrix(train_x), label = as.matrix(train_y))
xgb_test <- xgb.DMatrix(data = as.matrix(test_x))

# Model Building: XGBoost
param_list <- list(
  objective = "reg:squarederror",
  eta = 0.01,
  gamma = 1,
  max_depth = 6,
  subsample = 0.8,
  colsample_bytree = 0.5
)

# we see that 500 rounds is sufficient with this data
xgbcv <- xgb.cv(
  params = param_list,
  data = xgb_train,
  nrounds = 1000,
  nfold = 5,
  print_every_n = 10,
  early_stopping_rounds = 30,
  maximize = F
)

# define the final model
xgb_model <- xgb.train(
  data = xgb_train,
  params = param_list,
  nrounds = 1000
)

# Variable Importance
var_imp <- xgb.importance(
  feature_names = setdiff(
    names(train),
    c("Item_Identifier", "Item_Outlet_Sales")
  ),
  model = xgb_model
)

# Importance plot
xgb.plot.importance(var_imp)

print(var_imp)

prediction_table <- data.table(
  Id = encoded_test_table$Id, 
  SalePrice = predict(xgb_model, xgb_test)
  )

write.csv(
  prediction_table,
  file = "/kaggle/working/submission.csv", row.names = FALSE
)
